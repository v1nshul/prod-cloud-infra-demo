name: CD - Build and Deploy

# AWS Authentication Configuration
# This workflow uses static AWS credentials stored as GitHub Secrets:
#   - AWS_ACCESS_KEY_ID
#   - AWS_SECRET_ACCESS_KEY
#
# To migrate to OIDC authentication (recommended for production):
# 1. Configure OIDC in AWS IAM: https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/configuring-openid-connect-in-amazon-web-services
# 2. Update each "Configure AWS credentials" step to use:
#    - role-to-assume: arn:aws:iam::ACCOUNT_ID:role/GITHUB_ROLE_NAME
#    - role-session-name: GitHubActions-${{ github.run_id }}
#    - Remove: aws-access-key-id and aws-secret-access-key
# 3. Remove the corresponding secrets from GitHub repository settings
# 4. Update job permissions to include id-token: write
#
# Kubernetes Deployment Strategy
# This workflow uses SSH-based remote execution to run kubectl commands on the k3s EC2 instance.
# This approach:
#   - Avoids exposing kubeconfig files in GitHub Secrets
#   - Uses k3s's default kubeconfig at /etc/rancher/k3s/k3s.yaml automatically
#   - Ensures kubectl runs where k3s is installed, avoiding localhost:8080 issues
#   - Maintains security by not requiring direct Kubernetes API access from GitHub Actions
# Required secret: SSH_PRIVATE_KEY (from terraform output ssh_private_key_path)

on:
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: v1nshul/prod-cloud-infra-demo

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    outputs:
      image-digest: ${{ steps.build.outputs.digest }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    # Authenticate with GitHub Container Registry (GHCR)
    # Uses GITHUB_TOKEN which is automatically provided by GitHub Actions
    # No additional secrets required for GHCR authentication
    - name: Log in to GitHub Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=sha,prefix=,format=long
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Generate image tag
      id: image-tag
      run: |
        if [ "${{ github.ref }}" == "refs/heads/main" ]; then
          echo "tag=latest" >> $GITHUB_OUTPUT
        else
          echo "tag=${{ github.sha }}" >> $GITHUB_OUTPUT
        fi

    - name: Build and push Docker image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy-staging:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && (github.event_name == 'push' || github.event.inputs.environment == 'staging')
    environment:
      name: staging
      url: ${{ steps.deploy.outputs.url }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    # Configure AWS credentials using static credentials from GitHub Secrets
    # Required secrets: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
    # Note: For enhanced security, consider migrating to OIDC authentication in the future
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: eu-west-2
        # Export credentials to environment variables for subsequent steps
        output-env-credentials: true

    - name: Get k3s instance public IP
      id: get-ip
      run: |
        INSTANCE_IP=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=prod-cloud-infra-demo-k3s" "Name=instance-state-name,Values=running" \
          --query 'Reservations[0].Instances[0].PublicIpAddress' \
          --output text \
          --region eu-west-2)
        echo "instance_ip=$INSTANCE_IP" >> $GITHUB_OUTPUT
        echo "k3s instance IP: $INSTANCE_IP"

    # Setup SSH key for remote access to EC2 instance
    # Required secret: SSH_PRIVATE_KEY (the private key from terraform output ssh_private_key_path)
    - name: Setup SSH key
      run: |
        mkdir -p ~/.ssh
        echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/k3s_private_key.pem
        chmod 600 ~/.ssh/k3s_private_key.pem
        ssh-keyscan -H ${{ steps.get-ip.outputs.instance_ip }} >> ~/.ssh/known_hosts 2>/dev/null || true

    # Set up kustomize locally to build manifests before deploying
    - name: Set up kustomize
      uses: imranismail/setup-kustomize@v2
      with:
        version: '5.0.0'

    # Deploy to staging by building manifests locally and applying via SSH on the remote k3s server
    # This approach avoids exposing kubeconfig and ensures kubectl runs where k3s is installed
    # k3s's default kubeconfig at /etc/rancher/k3s/k3s.yaml is used automatically
    - name: Deploy to staging
      id: deploy
      run: |
        INSTANCE_IP="${{ steps.get-ip.outputs.instance_ip }}"
        SSH_KEY="$HOME/.ssh/k3s_private_key.pem"
        IMAGE_TAG="latest"
        
        if [ "${{ github.ref }}" != "refs/heads/main" ]; then
          IMAGE_TAG="${{ github.sha }}"
        fi
        
        # Build kustomize manifests locally with updated image tag
        echo "Building kustomize manifests with image tag: ${IMAGE_TAG}"
        cd k8s/staging
        kustomize edit set image ghcr.io/${{ env.IMAGE_NAME }}:${IMAGE_TAG}
        
        # Build the final manifests
        mkdir -p /tmp/k8s-built
        kustomize build . > /tmp/k8s-built/staging-manifests.yaml
        
        # Copy built manifests to the server
        echo "Copying built manifests to server..."
        scp -i "$SSH_KEY" /tmp/k8s-built/staging-manifests.yaml ec2-user@$INSTANCE_IP:/tmp/staging-manifests.yaml
        
        # SSH into the server and apply manifests
        # kubectl uses /etc/rancher/k3s/k3s.yaml automatically when run on the k3s server
        ssh -i "$SSH_KEY" ec2-user@$INSTANCE_IP bash -c "
          set -e
          
          # Apply manifests
          kubectl apply -f /tmp/staging-manifests.yaml
          
          # Wait for deployment rollout
          kubectl rollout status deployment/app -n prod-cloud-infra-demo-staging --timeout=5m
          
          echo 'Deployment completed successfully'
        "
        
        # Get application URL
        APP_URL="http://$INSTANCE_IP"
        echo "url=$APP_URL" >> $GITHUB_OUTPUT
        echo "Application deployed and accessible at: $APP_URL"

  deploy-production:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'production'
    environment:
      name: production
      url: ${{ steps.deploy.outputs.url }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    # Configure AWS credentials using static credentials from GitHub Secrets
    # Required secrets: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
    # Note: For enhanced security, consider migrating to OIDC authentication in the future
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: eu-west-2
        # Export credentials to environment variables for subsequent steps
        output-env-credentials: true

    - name: Get k3s instance public IP
      id: get-ip
      run: |
        INSTANCE_IP=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=prod-cloud-infra-demo-k3s" "Name=instance-state-name,Values=running" \
          --query 'Reservations[0].Instances[0].PublicIpAddress' \
          --output text \
          --region eu-west-2)
        echo "instance_ip=$INSTANCE_IP" >> $GITHUB_OUTPUT
        echo "k3s instance IP: $INSTANCE_IP"

    # Setup SSH key for remote access to EC2 instance
    # Required secret: SSH_PRIVATE_KEY (the private key from terraform output ssh_private_key_path)
    - name: Setup SSH key
      run: |
        mkdir -p ~/.ssh
        echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/k3s_private_key.pem
        chmod 600 ~/.ssh/k3s_private_key.pem
        ssh-keyscan -H ${{ steps.get-ip.outputs.instance_ip }} >> ~/.ssh/known_hosts 2>/dev/null || true

    # Set up kustomize locally to build manifests before deploying
    - name: Set up kustomize
      uses: imranismail/setup-kustomize@v2
      with:
        version: '5.0.0'

    # Deploy to production by building manifests locally and applying via SSH on the remote k3s server
    # This approach avoids exposing kubeconfig and ensures kubectl runs where k3s is installed
    # k3s's default kubeconfig at /etc/rancher/k3s/k3s.yaml is used automatically
    - name: Deploy to production
      id: deploy
      run: |
        INSTANCE_IP="${{ steps.get-ip.outputs.instance_ip }}"
        SSH_KEY="$HOME/.ssh/k3s_private_key.pem"
        IMAGE_TAG="latest"
        
        if [ "${{ github.ref }}" != "refs/heads/main" ]; then
          IMAGE_TAG="${{ github.sha }}"
        fi
        
        # Build kustomize manifests locally with updated image tag
        echo "Building kustomize manifests with image tag: ${IMAGE_TAG}"
        cd k8s/production
        kustomize edit set image ghcr.io/${{ env.IMAGE_NAME }}:${IMAGE_TAG}
        
        # Build the final manifests
        mkdir -p /tmp/k8s-built
        kustomize build . > /tmp/k8s-built/production-manifests.yaml
        
        # Copy built manifests to the server
        echo "Copying built manifests to server..."
        scp -i "$SSH_KEY" /tmp/k8s-built/production-manifests.yaml ec2-user@$INSTANCE_IP:/tmp/production-manifests.yaml
        
        # SSH into the server and apply manifests
        # kubectl uses /etc/rancher/k3s/k3s.yaml automatically when run on the k3s server
        ssh -i "$SSH_KEY" ec2-user@$INSTANCE_IP bash -c "
          set -e
          
          # Apply manifests
          kubectl apply -f /tmp/production-manifests.yaml
          
          # Wait for deployment rollout
          kubectl rollout status deployment/app -n prod-cloud-infra-demo --timeout=5m
          
          echo 'Deployment completed successfully'
        "
        
        # Get application URL
        APP_URL="http://$INSTANCE_IP"
        echo "url=$APP_URL" >> $GITHUB_OUTPUT
        echo "Application deployed and accessible at: $APP_URL"

  rollback:
    needs: [deploy-staging, deploy-production]
    runs-on: ubuntu-latest
    if: failure() && (needs.deploy-staging.result == 'failure' || needs.deploy-production.result == 'failure')
    
    steps:
    # Configure AWS credentials using static credentials from GitHub Secrets
    # Required secrets: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
    # Note: For enhanced security, consider migrating to OIDC authentication in the future
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: eu-west-2
        # Export credentials to environment variables for subsequent steps
        output-env-credentials: true

    - name: Get k3s instance public IP
      id: get-ip
      run: |
        INSTANCE_IP=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=prod-cloud-infra-demo-k3s" "Name=instance-state-name,Values=running" \
          --query 'Reservations[0].Instances[0].PublicIpAddress' \
          --output text \
          --region eu-west-2)
        echo "instance_ip=$INSTANCE_IP" >> $GITHUB_OUTPUT

    # Setup SSH key for remote access to EC2 instance
    # Required secret: SSH_PRIVATE_KEY (the private key from terraform output ssh_private_key_path)
    - name: Setup SSH key
      run: |
        mkdir -p ~/.ssh
        echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/k3s_private_key.pem
        chmod 600 ~/.ssh/k3s_privatekey.pem
        ssh-keyscan -H ${{ steps.get-ip.outputs.instance_ip }} >> ~/.ssh/known_hosts 2>/dev/null || true

    # Rollback deployment by executing kubectl commands on the remote k3s server
    # kubectl uses /etc/rancher/k3s/k3s.yaml automatically when run on the k3s server
    - name: Rollback deployment
      run: |
        INSTANCE_IP="${{ steps.get-ip.outputs.instance_ip }}"
        SSH_KEY="$HOME/.ssh/k3s_private_key.pem"
        
        # Determine which environment failed and rollback via SSH
        ssh -i "$SSH_KEY" ec2-user@$INSTANCE_IP bash -c "
          set -e
          
          if [ '${{ needs.deploy-staging.result }}' == 'failure' ]; then
            echo 'Rolling back staging deployment...'
            kubectl rollout undo deployment/app -n prod-cloud-infra-demo-staging
            kubectl rollout status deployment/app -n prod-cloud-infra-demo-staging --timeout=5m
            echo 'Rolled back staging deployment'
          fi
          
          if [ '${{ needs.deploy-production.result }}' == 'failure' ]; then
            echo 'Rolling back production deployment...'
            kubectl rollout undo deployment/app -n prod-cloud-infra-demo
            kubectl rollout status deployment/app -n prod-cloud-infra-demo --timeout=5m
            echo 'Rolled back production deployment'
          fi
        "
